# Quickstart: RAG Vector Database with Chunking

## Prerequisites

1. **Ollama installed and running locally**
   - Install from https://ollama.ai
   - Start Ollama service: `ollama serve` (or ensure it's running as a service)
   - Pull required models:
     ```bash
     ollama pull embeddinggemma:300m   # or your preferred embedding model
     ollama pull gemma3:12b            # or your preferred query model
     ```

2. **Python 3.13 environment**
   - Project dependencies will be installed via `uv` or `pip`

3. **Markdown files to process**
   - Files generated by feature 001-setup-docling in `data/output/`
   - Or any `.md` files you want to index

## Setup

1. **Install dependencies**
   ```bash
   # Add LangChain and related packages to pyproject.toml dependencies
   uv sync  # or pip install -e .
   ```

2. **Configure environment variables**
   Create a `.env` file in the project root:
   ```env
   OLLAMA_EMBEDDING_MODEL=embeddinggemma:300m
   OLLAMA_QUERY_MODEL=gemma3:12b
   CHUNK_SIZE=1000
   CHUNK_OVERLAP=200
   RETRIEVER_TOP_K=4
   RETRIEVER_MIN_SIMILARITY=0.0
   ```

3. **Verify Ollama models are available**
   ```bash
   ollama list
   # Should show embeddinggemma:300m and gemma3:12b (or your chosen models)
   ```

## Usage

### Step 1: Process Markdown Files

Process a single file:
```bash
pdf-rag process data/output/DC20_Beta_0_9.md
```

Process all files in a directory:
```bash
pdf-rag process data/output/
```

Expected output:
```
Processing Markdown files...
- DC20_Beta_0_9.md: Added 234 chunks (0 skipped)
- OSW_DigitalBook_Pages.md: Added 567 chunks (0 skipped)
- STC_Stormlight_Handbook_digital.md: Added 412 chunks (0 skipped)

Summary: Processed 3 files | Added 1213 chunks | Skipped 0 chunks
Database location: data/db/
```

### Step 2: Query the Database

Ask a question about the processed documents:
```bash
pdf-rag query "What is the main topic of the documents?"
```

Expected output (answer text only):
```
[Generated answer based on relevant chunks from the documents]
```

### Step 3: Process Additional Files (with Deduplication)

If you process the same file again, duplicates are automatically skipped:
```bash
pdf-rag process data/output/DC20_Beta_0_9.md
```

Expected output:
```
Processing Markdown files...
- DC20_Beta_0_9.md: Added 0 chunks (234 skipped - duplicates)

Summary: Processed 1 file | Added 0 chunks | Skipped 234 chunks
Database location: data/db/
```

## Customizing Configuration

Edit `.env` to adjust chunking or retrieval parameters:

```env
# Larger chunks for more context
CHUNK_SIZE=2000
CHUNK_OVERLAP=400

# Retrieve more chunks for complex queries
RETRIEVER_TOP_K=8

# Require higher similarity for stricter filtering
RETRIEVER_MIN_SIMILARITY=0.3
```

## Troubleshooting

### Error: "Missing required environment variable"
- Ensure `.env` file exists in project root
- Check that `OLLAMA_EMBEDDING_MODEL` and `OLLAMA_QUERY_MODEL` are set

### Error: "Ollama model '<model>' not found"
- Verify Ollama is running: `ollama list`
- Pull the missing model: `ollama pull <model-name>`
- Check model name spelling in `.env`

### Error: "Vector database not found"
- Run `pdf-rag process` first to create the database
- Check that `data/db/` directory is writable

### Error: "No relevant chunks found"
- Lower `RETRIEVER_MIN_SIMILARITY` in `.env` (default is 0.0)
- Increase `RETRIEVER_TOP_K` to retrieve more chunks
- Verify documents contain relevant content for your query

### Query returns irrelevant answers
- Adjust `RETRIEVER_MIN_SIMILARITY` to filter low-quality chunks
- Increase `RETRIEVER_TOP_K` to provide more context to the LLM
- Try different query models (larger models often give better answers)

## Next Steps

- Process multiple document collections into separate databases (use `--db-path`)
- Experiment with different embedding models for better semantic understanding
- Fine-tune chunk size and overlap for your document types
- Adjust retrieval parameters based on query complexity

## Performance Tips

- **Large files**: Processing time scales with file size. A 5000-line file may take 1-2 minutes.
- **Many files**: Process files in batches if needed (system handles 10+ files without issues).
- **Query speed**: First query may be slower (model loading), subsequent queries are faster (<5 seconds).

